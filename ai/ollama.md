# Ollama

Seems like a sort of "docker for AI models"

## Modelfile

- works similar to `Dockerfile`
- docs: https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md
- You start with a base model and then can tweak it e.g.
    - change parameters
    - set a ssytem prompt
    - set a template of the full prompt to be passed to the model
        - should contain space for the system prompt (represented by `.System`
          variable)

```bash
❯ ollama list
NAME                    ID              SIZE    MODIFIED
eoin-mario:latest       27786c5f5927    3.8 GB  8 minutes ago
llama2:latest           fe938a131f40    3.8 GB  13 minutes ago

❯ ollama show --modelfile llama2:latest
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM llama2:latest

FROM /Users/eoinkelly/.ollama/models/blobs/sha256:longshahere

# https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#template
TEMPLATE """[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
"""

# PARAMETER docs
# https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#parameter

# Sets the size of the context window used to generate the next token. (Default: 2048)
PARAMETER num_ctx 4096

# Sets the stop sequences to use. When this pattern is encountered the LLM will
# stop generating text and return. Multiple stop patterns may be set by
# specifying multiple separate stop parameters in a modelfile.
PARAMETER stop "[INST]"
PARAMETER stop "[/INST]"
PARAMETER stop "<<SYS>>"
PARAMETER stop "<</SYS>>"
```

```Modelfile
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM llama2:latest

FROM /Users/eoinkelly/.ollama/models/blobs/sha256:longshahere

# https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#template
TEMPLATE """[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
"""

# PARAMETER docs
# https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#parameter

# Sets the size of the context window used to generate the next token. (Default: 2048)
PARAMETER num_ctx 4096

# Sets the stop sequences to use. When this pattern is encountered the LLM will
# stop generating text and return. Multiple stop patterns may be set by
# specifying multiple separate stop parameters in a modelfile.
PARAMETER stop "[INST]"
PARAMETER stop "[/INST]"
PARAMETER stop "<<SYS>>"
PARAMETER stop "<</SYS>>"
```
